{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Final-bert.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjNMvDqYqle0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUTPath = './output/'\n",
        "!mkdir ./output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O2CzZzctzWe",
        "colab_type": "code",
        "outputId": "de2efdd5-ab59-431a-f0f3-37158a6911b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "# Save model to your Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0nMqQaqrdRT",
        "colab_type": "text"
      },
      "source": [
        "#Prepare Pretrained Bert model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRjRbjTUqCT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "e2d49a59-e11b-430d-cc79-272ed5a8a4b5"
      },
      "source": [
        "!pip install -q keras-bert\n",
        "!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip -o uncased_L-12_H-768_A-12.zip -d './gdrive/My Drive/NLP/Final-Project/data/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "  inflating: ./gdrive/My Drive/NLP/Final-Project/data/uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: ./gdrive/My Drive/NLP/Final-Project/data/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: ./gdrive/My Drive/NLP/Final-Project/data/uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: ./gdrive/My Drive/NLP/Final-Project/data/uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: ./gdrive/My Drive/NLP/Final-Project/data/uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv387qMNqrm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4406
        },
        "outputId": "13456a84-ebcf-483b-91d2-b0d4ea704209"
      },
      "source": [
        "from keras_bert import load_vocabulary, load_trained_model_from_checkpoint, Tokenizer, get_checkpoint_paths\n",
        "bert_path = './gdrive/My Drive/NLP/Final-Project/data/uncased_L-12_H-768_A-12'\n",
        "paths = get_checkpoint_paths(bert_path)\n",
        "model = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=83)\n",
        "model.summary(line_length=120)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0622 14:51:20.410980 140087159367552 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0622 14:51:20.453547 140087159367552 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0622 14:51:20.504328 140087159367552 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0622 14:51:20.505295 140087159367552 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0622 14:51:20.513640 140087159367552 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0622 14:51:20.543318 140087159367552 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0622 14:51:24.106881 140087159367552 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________________________________________________________________________\n",
            "Layer (type)                           Output Shape               Param #       Connected to                            \n",
            "========================================================================================================================\n",
            "Input-Token (InputLayer)               (None, 83)                 0                                                     \n",
            "________________________________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)             (None, 83)                 0                                                     \n",
            "________________________________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding)       [(None, 83, 768), (30522,  23440896      Input-Token[0][0]                       \n",
            "________________________________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)          (None, 83, 768)            1536          Input-Segment[0][0]                     \n",
            "________________________________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)          (None, 83, 768)            0             Embedding-Token[0][0]                   \n",
            "                                                                                Embedding-Segment[0][0]                 \n",
            "________________________________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmbedding) (None, 83, 768)            63744         Embedding-Token-Segment[0][0]           \n",
            "________________________________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)            (None, 83, 768)            0             Embedding-Position[0][0]                \n",
            "________________________________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalization)    (None, 83, 768)            1536          Embedding-Dropout[0][0]                 \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Embedding-Norm[0][0]                    \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-1-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Embedding-Norm[0][0]                    \n",
            "                                                                                Encoder-1-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-1-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-1-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-1-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-1-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-1-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-1-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Encoder-1-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-2-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Encoder-1-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-2-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-2-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-2-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-2-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-2-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-2-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-2-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Encoder-2-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-3-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Encoder-2-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-3-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-3-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-3-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-3-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-3-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-3-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-3-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Encoder-3-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-4-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Encoder-3-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-4-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-4-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-4-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-4-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-4-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-4-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-4-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Encoder-4-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-5-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Encoder-4-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-5-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-5-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-5-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-5-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-5-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-5-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-5-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Encoder-5-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-6-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Encoder-5-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-6-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-6-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-6-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-6-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-6-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-6-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-6-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Encoder-6-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-7-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Encoder-6-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-7-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-7-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-7-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-7-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-7-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-7-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-7-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Encoder-7-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-8-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Encoder-7-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-8-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-8-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-8-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-8-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-8-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-8-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-8-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttention (Mult (None, 83, 768)            2362368       Encoder-8-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttention-Dropo (None, 83, 768)            0             Encoder-9-MultiHeadSelfAttention[0][0]  \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttention-Add ( (None, 83, 768)            0             Encoder-8-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-9-MultiHeadSelfAttention-Dropout\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttention-Norm  (None, 83, 768)            1536          Encoder-9-MultiHeadSelfAttention-Add[0][\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForward)    (None, 83, 768)            4722432       Encoder-9-MultiHeadSelfAttention-Norm[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout (Dropout (None, 83, 768)            0             Encoder-9-FeedForward[0][0]             \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add)        (None, 83, 768)            0             Encoder-9-MultiHeadSelfAttention-Norm[0]\n",
            "                                                                                Encoder-9-FeedForward-Dropout[0][0]     \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (LayerNorma (None, 83, 768)            1536          Encoder-9-FeedForward-Add[0][0]         \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttention (Mul (None, 83, 768)            2362368       Encoder-9-FeedForward-Norm[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttention-Drop (None, 83, 768)            0             Encoder-10-MultiHeadSelfAttention[0][0] \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttention-Add  (None, 83, 768)            0             Encoder-9-FeedForward-Norm[0][0]        \n",
            "                                                                                Encoder-10-MultiHeadSelfAttention-Dropou\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttention-Norm (None, 83, 768)            1536          Encoder-10-MultiHeadSelfAttention-Add[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedForward)   (None, 83, 768)            4722432       Encoder-10-MultiHeadSelfAttention-Norm[0\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout (Dropou (None, 83, 768)            0             Encoder-10-FeedForward[0][0]            \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add)       (None, 83, 768)            0             Encoder-10-MultiHeadSelfAttention-Norm[0\n",
            "                                                                                Encoder-10-FeedForward-Dropout[0][0]    \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (LayerNorm (None, 83, 768)            1536          Encoder-10-FeedForward-Add[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttention (Mul (None, 83, 768)            2362368       Encoder-10-FeedForward-Norm[0][0]       \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttention-Drop (None, 83, 768)            0             Encoder-11-MultiHeadSelfAttention[0][0] \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttention-Add  (None, 83, 768)            0             Encoder-10-FeedForward-Norm[0][0]       \n",
            "                                                                                Encoder-11-MultiHeadSelfAttention-Dropou\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttention-Norm (None, 83, 768)            1536          Encoder-11-MultiHeadSelfAttention-Add[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedForward)   (None, 83, 768)            4722432       Encoder-11-MultiHeadSelfAttention-Norm[0\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout (Dropou (None, 83, 768)            0             Encoder-11-FeedForward[0][0]            \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add)       (None, 83, 768)            0             Encoder-11-MultiHeadSelfAttention-Norm[0\n",
            "                                                                                Encoder-11-FeedForward-Dropout[0][0]    \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (LayerNorm (None, 83, 768)            1536          Encoder-11-FeedForward-Add[0][0]        \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttention (Mul (None, 83, 768)            2362368       Encoder-11-FeedForward-Norm[0][0]       \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttention-Drop (None, 83, 768)            0             Encoder-12-MultiHeadSelfAttention[0][0] \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttention-Add  (None, 83, 768)            0             Encoder-11-FeedForward-Norm[0][0]       \n",
            "                                                                                Encoder-12-MultiHeadSelfAttention-Dropou\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttention-Norm (None, 83, 768)            1536          Encoder-12-MultiHeadSelfAttention-Add[0]\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedForward)   (None, 83, 768)            4722432       Encoder-12-MultiHeadSelfAttention-Norm[0\n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout (Dropou (None, 83, 768)            0             Encoder-12-FeedForward[0][0]            \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add)       (None, 83, 768)            0             Encoder-12-MultiHeadSelfAttention-Norm[0\n",
            "                                                                                Encoder-12-FeedForward-Dropout[0][0]    \n",
            "________________________________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (LayerNorm (None, 83, 768)            1536          Encoder-12-FeedForward-Add[0][0]        \n",
            "========================================================================================================================\n",
            "Total params: 108,562,176\n",
            "Trainable params: 0\n",
            "Non-trainable params: 108,562,176\n",
            "________________________________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_eXuLy-rs6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dict = load_vocabulary(paths.vocab)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYlv3diesMkf",
        "colab_type": "text"
      },
      "source": [
        "#Load clean text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHX4WbJeoJnd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "42b73f03-d63c-48b7-d8f6-b7faa57dfe43"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "DATA_SET_DIR = './gdrive/My Drive/NLP/Final-Project/data/'\n",
        "TRAINING_DATA_PATH = DATA_SET_DIR+'olid-training-clean.csv'\n",
        "TEST_PATH_DICT = {}\n",
        "TEST_A_DATA_PATH = DATA_SET_DIR+'testset-levela-clean.tsv'\n",
        "TEST_B_DATA_PATH = DATA_SET_DIR+'testset-levelb-clean.tsv'\n",
        "TEST_C_DATA_PATH = DATA_SET_DIR+'testset-levelc-clean.tsv'\n",
        "TEST_PATH_DICT['subtask_a'] = TEST_A_DATA_PATH\n",
        "TEST_PATH_DICT['subtask_b'] = TEST_B_DATA_PATH\n",
        "TEST_PATH_DICT['subtask_c'] = TEST_C_DATA_PATH\n",
        "\n",
        "data={}\n",
        "data['train'] = {}\n",
        "data['validation'] = {}\n",
        "data['test'] = {}\n",
        "task_list = ['subtask_a','subtask_b','subtask_c']\n",
        "train = pd.read_csv(TRAINING_DATA_PATH,sep='\\t', index_col=False)\n",
        "for subtask in task_list:\n",
        "    data['test'][subtask] = pd.read_csv(TEST_PATH_DICT[subtask],sep='\\t', index_col=False)\n",
        "train.head(3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet_exp</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90194</td>\n",
              "      <td>@USER @USER Go home you are drunk!!! @USER #MA...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16820</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                          tweet_exp  ... subtask_b subtask_c\n",
              "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
              "1  90194  @USER @USER Go home you are drunk!!! @USER #MA...  ...       TIN       IND\n",
              "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXn-FAOjsi_5",
        "colab_type": "text"
      },
      "source": [
        "#Text Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgPCsvuAsZpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(text):\n",
        "#     def handle_emoji(text):\n",
        "#       # Dictionnary of \"important\" emojis : \n",
        "#       emoji_dict =  {'♥️': ' love ',\n",
        "#                      '❤️' : ' love ',\n",
        "#                      '❤' : ' love ',\n",
        "#                      '😘' : ' kisses ',\n",
        "#                     '😭' : ' cry ',\n",
        "#                     '💪' : ' strong ',\n",
        "#                     '🌍' : ' earth ',\n",
        "#                     '💰' : ' money ',\n",
        "#                     '👍' : ' ok ',\n",
        "#                      '👌' : ' ok ',\n",
        "#                     '😡' : ' angry ',\n",
        "#                     '🍆' : ' dick ',\n",
        "#                     '🤣' : ' haha ',\n",
        "#                     '😂' : ' haha ',\n",
        "#                     '🖕' : ' fuck you '}\n",
        "\n",
        "#       for cha in emoji_dict:\n",
        "#           text = re.compile(str(cha)).sub(str(emoji_dict[cha]),text)\n",
        "#       # Remove ALL emojis\n",
        "#       text = emoji.get_emoji_regexp().sub(r' ',text) \n",
        "#       text = re.compile(\"([\\U0001f3fb-\\U0001f3ff])\").sub(r'',text) \n",
        "#       text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r'',text) \n",
        "#       text = re.compile(\"(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\").sub(r'',text)\n",
        "\n",
        "#       # Add Space between  the Emoji Expressions : \n",
        "#       text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r' \\1 ',text) \n",
        "#       return text\n",
        "    ''' Function that is applied to every to tweet in the dataset '''\n",
        "\n",
        "    # =========== TEXT ===========\n",
        "    # Replace @USER by <user>\n",
        "    text = re.compile(r'@USER').sub(r'<user>',text)\n",
        "\n",
        "    # Replace URL by <url>\n",
        "    text = re.compile(r'URL').sub(r'<url>',text)\n",
        "\n",
        "    # Remove numbers :\n",
        "    text = re.compile(r'[0-9]+').sub(r' ',text)\n",
        "\n",
        "    # Remove some special characters\n",
        "    text = re.compile(r'([\\xa0_\\{\\}\\[\\]¬•$,:;/@#|\\^*%().~`”\"“-])').sub(r' ',text) \n",
        "\n",
        "    # Space the special characters with white spaces\n",
        "    text = re.compile(r'([$&+,:;=?@#|.^*()%!\"’“-])').sub(r' \\1 ',text)\n",
        "\n",
        "    # Replace some special characters : \n",
        "    replace_dict = {r'&' : 'and' , \n",
        "                    r'\\+' : 'plus'}\n",
        "    for cha in replace_dict:\n",
        "        text = re.compile(str(cha)).sub(str(replace_dict[cha]),text)\n",
        "\n",
        "    # Handle Emoji : translate some and delete the others\n",
        "#     text = handle_emoji(text)\n",
        "\n",
        "    # Word delengthening : \n",
        "    text = re.compile(r'(.)\\1{3,}').sub(r'\\1\\1',text)\n",
        "\n",
        "    # Cut the words with caps in them : \n",
        "    text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)\n",
        "    text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)        \n",
        "    # =========== TOKENS ===========\n",
        "    # TOKENIZE \n",
        "    text = text.split(' ')\n",
        "\n",
        "    # Remove white spaces tokens\n",
        "    text = [text[i] for i in range(len(text)) if text[i] != ' ']\n",
        "\n",
        "    # Remove empty tokens\n",
        "    text = [text[i] for i in range(len(text)) if text[i] != '']\n",
        "\n",
        "    # Remove repetition in tokens (!!! => !)\n",
        "    text = [text[i] for i in range(len(text)) if text[i] != text[i-1]]\n",
        "\n",
        "    #  Handle the ALL CAPS Tweets \n",
        "    ### if ratio of caps in the word > 75% add allcaps tag <allcaps>\n",
        "    caps_r = np.mean([text[i].isupper() for i in range(len(text))])\n",
        "    if caps_r > 0.6 : \n",
        "        text.append('<allcaps>')\n",
        "\n",
        "    # Lower Case : \n",
        "    text = [text[i].lower() for i in range(len(text))]\n",
        "\n",
        "    return ' '.join(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdkS3Y7YspQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72b7ec9b-5261-4398-d5ce-989498a95757"
      },
      "source": [
        "import re, string\n",
        "def _run_split_on_punc(text):\n",
        "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "    chars = list(text)\n",
        "    i = 0\n",
        "    start_new_word = True\n",
        "    output = []\n",
        "    def _is_punctuation(char):\n",
        "        # print('[%s]+' % re.escape(string.punctuation))\n",
        "        if re.match(r'[\\!\\\"\\$\\%\\&\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\[\\\\\\]\\^_\\`\\{\\|\\}\\~]+',char):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    while i < len(chars):\n",
        "        char = chars[i]\n",
        "        if _is_punctuation(char):\n",
        "            output.append([char])\n",
        "            start_new_word = True\n",
        "        else:\n",
        "            if start_new_word:\n",
        "                output.append([])\n",
        "            start_new_word = False\n",
        "            output[-1].append(char)\n",
        "        i += 1\n",
        "    text_list = [\"\".join(x) for x in output]\n",
        "    text = ' '.join(text_list)\n",
        "    text = text.split(' ')\n",
        "    return ' '.join(text)\n",
        "print(_run_split_on_punc(\"@USER @USER @USER @USER LOL!!!   Throwing the BULLSHIT Flag on such nonsense!!  #PutUpOrShutUp\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@USER @USER @USER @USER LOL ! ! !    Throwing the BULLSHIT Flag on such nonsense ! !   #PutUpOrShutUp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlJ1wEe4suXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.tweet_exp = train.tweet_exp.apply(_run_split_on_punc)\n",
        "train.tweet_exp = train.tweet_exp.apply(clean_tweet)\n",
        "for subtask in task_list:\n",
        "    data['test'][subtask].tweet_exp = data['test'][subtask].tweet_exp.apply(_run_split_on_punc)\n",
        "    data['test'][subtask].tweet_exp = data['test'][subtask].tweet_exp.apply(clean_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE5HRpRcte3w",
        "colab_type": "text"
      },
      "source": [
        "#Prepare Bert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUb6tbJMsxdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert\n",
        "token_dict = load_vocabulary(paths.vocab)\n",
        "xtrain_list = train.tweet_exp.tolist()\n",
        "tokenizer = Tokenizer(token_dict)\n",
        "indice=[]\n",
        "segments = []\n",
        "for line in xtrain_list:\n",
        "#   tokens = tokenizer.tokenize(line)\n",
        "  idx, seg = tokenizer.encode(first=line, max_len=83)\n",
        "  indice.append(idx)\n",
        "  segments.append(seg)\n",
        "predicts1 = model.predict([np.array(indice[:5000]), np.array(segments[:5000])])\n",
        "predicts2 = model.predict([np.array(indice[5000:]), np.array(segments[5000:])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esWwcDvJt319",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7465827c-dbaf-4f07-f6bd-fef03038fcac"
      },
      "source": [
        "predicts = np.concatenate((predicts1,predicts2),axis=0)\n",
        "predicts.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13240, 83, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ9gD1GuuZD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('./gdrive/My Drive/NLP/Final-Project/data/train_bert_vector',predicts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngYBFJHtufxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del predicts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtx2g3h8ujYQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b44a777-63b8-493b-ac10-f7ba41776763"
      },
      "source": [
        "for subtask in task_list:\n",
        "    data['test'][subtask+'_list'] = data['test'][subtask].tweet_exp.tolist()\n",
        "    indice=[]\n",
        "    segments = []\n",
        "    for line in data['test'][subtask+'_list']:\n",
        "    #   tokens = tokenizer.tokenize(line)\n",
        "      idx, seg = tokenizer.encode(first=line, max_len=83)\n",
        "      indice.append(idx)\n",
        "      segments.append(seg)\n",
        "    predicts = model.predict([np.array(indice), np.array(segments)])\n",
        "    print(predicts.shape)\n",
        "    np.save('./gdrive/My Drive/NLP/Final-Project/data/test_bert_vector_'+subtask,predicts)\n",
        "    del predicts"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(860, 83, 768)\n",
            "(240, 83, 768)\n",
            "(213, 83, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgcCwYiTvHbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KqEbZ8Vu1xd",
        "colab_type": "text"
      },
      "source": [
        "#Load Bert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUcb8XvMs3vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "DATA_SET_DIR = './gdrive/My Drive/NLP/Final-Project/data/'\n",
        "TRAINING_DATA_PATH = DATA_SET_DIR+'train_bert_vector.npy'\n",
        "TEST_PATH_DICT = {}\n",
        "TEST_A_DATA_PATH = DATA_SET_DIR+'test_bert_vector_subtask_a.npy'\n",
        "TEST_B_DATA_PATH = DATA_SET_DIR+'test_bert_vector_subtask_b.npy'\n",
        "TEST_C_DATA_PATH = DATA_SET_DIR+'test_bert_vector_subtask_c.npy'\n",
        "TEST_PATH_DICT['subtask_a'] = TEST_A_DATA_PATH\n",
        "TEST_PATH_DICT['subtask_b'] = TEST_B_DATA_PATH\n",
        "TEST_PATH_DICT['subtask_c'] = TEST_C_DATA_PATH\n",
        "\n",
        "bert_data={}\n",
        "bert_data['train'] = {}\n",
        "bert_data['validation'] = {}\n",
        "bert_data['test'] = {}\n",
        "bert_data['train'] = np.load(TRAINING_DATA_PATH)\n",
        "for subtask in task_list:\n",
        "    bert_data['test'][subtask] = np.load(TEST_PATH_DICT[subtask])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quvIzipPmqQp",
        "colab_type": "code",
        "outputId": "0113ae51-947d-4cc6-b56c-b2ee3c672fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "classes_dict = {}\n",
        "classes_dict['subtask_a'] = {'NOT' : 0 ,'OFF' : 1}\n",
        "classes_dict['subtask_b'] = {'UNT' : 0 ,'TIN' : 1}\n",
        "classes_dict['subtask_c'] = {'IND' : 0 ,'OTH' : 1, 'GRP' : 2}\n",
        "for subtask in task_list:\n",
        "  data['train'][subtask]=pd.DataFrame()\n",
        "  idx = train[train[subtask].notnull()]\n",
        "  data['train'][subtask+'_idx'] = idx.index\n",
        "  data['train'][subtask]= idx[subtask]\n",
        "#     data['train'][subtask]=[]\n",
        "#     for label in classes_dict[subtask]:\n",
        "\n",
        "#         data['train'][subtask].append(data_label.index)\n",
        "#     data['train'][subtask] = \n",
        "#     print(subtask,\":\")\n",
        "  print(data['train'][subtask].value_counts())\n",
        "#     print(len(data['train'][subtask]))\n",
        "#     print('-'*10)\n",
        "data['train']['subtask_b'].head(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NOT    8840\n",
            "OFF    4400\n",
            "Name: subtask_a, dtype: int64\n",
            "TIN    3876\n",
            "UNT     524\n",
            "Name: subtask_b, dtype: int64\n",
            "IND    2407\n",
            "GRP    1074\n",
            "OTH     395\n",
            "Name: subtask_c, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    UNT\n",
              "1    TIN\n",
              "3    UNT\n",
              "5    TIN\n",
              "6    UNT\n",
              "Name: subtask_b, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HC4lrvluiVs",
        "colab_type": "code",
        "outputId": "f9a31d2b-9284-4f9d-b3d1-52cede393fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# 將分類標籤對應到剛定義的數字\n",
        "for subtask in task_list:\n",
        "  data['train'][subtask+'_y'] = data['train'][subtask].apply(lambda x: classes_dict[subtask][x])\n",
        "  print(data['train'][subtask+'_y'].value_counts())\n",
        "  data['train'][subtask+'_y'] = np.asarray(data['train'][subtask+'_y']).astype('float32')\n",
        "  print(data['train'][subtask+'_y'][:5])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    8840\n",
            "1    4400\n",
            "Name: subtask_a, dtype: int64\n",
            "[1. 1. 0. 1. 0.]\n",
            "1    3876\n",
            "0     524\n",
            "Name: subtask_b, dtype: int64\n",
            "[0. 1. 0. 1. 0.]\n",
            "0    2407\n",
            "2    1074\n",
            "1     395\n",
            "Name: subtask_c, dtype: int64\n",
            "[0. 1. 2. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkiBbKBvv4Gj",
        "colab_type": "text"
      },
      "source": [
        "#Prepare Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ2bYB1humkU",
        "colab_type": "code",
        "outputId": "95b82260-0f01-4a70-c1ef-eb3c0078893c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "NUM_LSTM_UNITS = 128\n",
        "# y_train = keras.utils.to_categorical(y_train)\n",
        "\n",
        "VALIDATION_RATIO = 0.15\n",
        "\n",
        "RANDOM_STATE = 9527\n",
        "for subtask in task_list:\n",
        "#   if subtask == 'subtask_c':\n",
        "#     data['train'][subtask+'_y'] = keras.utils.to_categorical(data['train'][subtask+'_y'])\n",
        "  \n",
        "  data['train'][subtask+'_x_train'], data['validation'][subtask+'_x_val'], \\\n",
        "  data['train'][subtask+'_y_train'], data['validation'][subtask+'_y_val'] =train_test_split(data['train'][subtask+'_idx'],data['train'][subtask+'_y'],test_size=VALIDATION_RATIO,random_state=RANDOM_STATE)\n",
        "  print(\"Training Set\",'----',subtask)\n",
        "  print(\"-\" * 10)\n",
        "  print(\"x_train: \", data['train'][subtask+'_x_train'].shape)\n",
        "  print(\"y_train : \", data['train'][subtask+'_y_train'].shape)\n",
        "\n",
        "  print(\"-\" * 10)\n",
        "  print(\"x_val:   \", data['validation'][subtask+'_x_val'].shape)\n",
        "  print(\"y_val :   \", data['validation'][subtask+'_y_val'].shape)\n",
        "  print(\"#\" * 10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set ---- subtask_a\n",
            "----------\n",
            "x_train:  (11254,)\n",
            "y_train :  (11254,)\n",
            "----------\n",
            "x_val:    (1986,)\n",
            "y_val :    (1986,)\n",
            "##########\n",
            "Training Set ---- subtask_b\n",
            "----------\n",
            "x_train:  (3740,)\n",
            "y_train :  (3740,)\n",
            "----------\n",
            "x_val:    (660,)\n",
            "y_val :    (660,)\n",
            "##########\n",
            "Training Set ---- subtask_c\n",
            "----------\n",
            "x_train:  (3294,)\n",
            "y_train :  (3294,)\n",
            "----------\n",
            "x_val:    (582,)\n",
            "y_val :    (582,)\n",
            "##########\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn3qyFOKwCHs",
        "colab_type": "text"
      },
      "source": [
        "#Upsampling (prepare balanced data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0vOzk0i4EYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "e505e895-18bb-4d6c-bccb-fec983a15fdd"
      },
      "source": [
        "def balanceData(subtask):\n",
        "    ''' Augments the Data given in input in order to balance the dataset'''\n",
        "    class_size = {}\n",
        "    for label in classes_dict[subtask]:\n",
        "        class_size[label] = len(data['train'][subtask+'_x_train'][data['train'][subtask+'_y_train']==classes_dict[subtask][label]])\n",
        "        print(classes_dict[subtask][label])\n",
        "    largest_class = max(class_size, key=class_size.get)\n",
        "    print(largest_class)\n",
        "    print('---- Augmenting the Data : ')\n",
        "    print('Before Augmentation : ',class_size)\n",
        "\n",
        "    for label in classes_dict[subtask]:  \n",
        "        if label != largest_class:\n",
        "            id_list = np.where(data['train'][subtask+'_y_train']==classes_dict[subtask][label])[0]\n",
        "            nb_augmentation = class_size[largest_class] - class_size[label]\n",
        "            id_augmentation = np.random.choice(id_list, (nb_augmentation,))\n",
        "            data['train'][subtask+'_x_train']=np.append(data['train'][subtask+'_x_train'],data['train'][subtask+'_x_train'][id_augmentation],axis=0)\n",
        "            data['train'][subtask+'_y_train']=np.append(data['train'][subtask+'_y_train'],data['train'][subtask+'_y_train'][id_augmentation],axis=0)\n",
        "    # Check if it went well\n",
        "    for label in classes_dict[subtask]:\n",
        "        class_size[label] = len(data['train'][subtask+'_x_train'][data['train'][subtask+'_y_train']==classes_dict[subtask][label]])\n",
        "\n",
        "    print('After Augmentation : ',class_size)\n",
        "    return data\n",
        "for subtask in task_list:\n",
        "  balanceData(subtask)\n",
        "#   balanceData(data['train'][subtask],subtask)\n",
        "  class_size = {}\n",
        "  for label in classes_dict[subtask]:\n",
        "    class_size[label] = len(data['train'][subtask+'_y_train'][data['train'][subtask+'_y_train']==classes_dict[subtask][label]])\n",
        "#   print('**After Augmentation : ',class_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "NOT\n",
            "---- Augmenting the Data : \n",
            "Before Augmentation :  {'NOT': 7492, 'OFF': 3762}\n",
            "After Augmentation :  {'NOT': 7492, 'OFF': 7492}\n",
            "0\n",
            "1\n",
            "TIN\n",
            "---- Augmenting the Data : \n",
            "Before Augmentation :  {'UNT': 452, 'TIN': 3288}\n",
            "After Augmentation :  {'UNT': 3288, 'TIN': 3288}\n",
            "0\n",
            "1\n",
            "2\n",
            "IND\n",
            "---- Augmenting the Data : \n",
            "Before Augmentation :  {'IND': 2033, 'OTH': 339, 'GRP': 922}\n",
            "After Augmentation :  {'IND': 2033, 'OTH': 2033, 'GRP': 2033}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANszWySmDd4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "data['train']['subtask_c'+'_y_train'] = keras.utils.to_categorical(data['train']['subtask_c'+'_y_train'])\n",
        "data['validation']['subtask_c'+'_y_val'] = keras.utils.to_categorical(data['validation']['subtask_c'+'_y_val'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MFt_nW_u8or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "import pickle\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88cHQRFMwVgt",
        "colab_type": "text"
      },
      "source": [
        "#Customized Callback Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X9M7HFhvBbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score,accuracy_score\n",
        "class Metrics(Callback):\n",
        "    def __init__(self, filepath,validation_data=(),IsBinaryClass=True):\n",
        "        self.file_path = filepath\n",
        "        self.X_val, self.y_val = validation_data\n",
        "        if not IsBinaryClass:\n",
        "          self.y_val = np.argmax(self.y_val,axis=1)\n",
        "        self.IsBinaryClass_ = IsBinaryClass\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.val_f1s = []\n",
        "        self.best_val_f1 = 0\n",
        "        self.val_recalls = []\n",
        "        self.val_precisions = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if self.IsBinaryClass_:\n",
        "          val_predict = (np.asarray(self.model.predict(self.X_val))).round().astype('int').reshape(-1)\n",
        "        else:\n",
        "          val_predict = np.argmax(self.model.predict(self.X_val),axis=1)\n",
        "        val_targ = self.y_val\n",
        "        _val_f1 = f1_score(val_targ, val_predict,average='macro')\n",
        "#         _val_recall = recall_score(val_targ, val_predict)\n",
        "#         _val_precision = precision_score(val_targ, val_predict)\n",
        "        self.val_f1s.append(_val_f1)\n",
        "#         self.val_recalls.append(_val_recall)\n",
        "#         self.val_precisions.append(_val_precision)\n",
        "#         print(' — val_f1: %f — val_precision: %f — val_recall %f' %(_val_f1, _val_precision, _val_recall))\n",
        "        \n",
        "        print(' — val_f1: %f' %(_val_f1))\n",
        "        print(\"max f1; \",max(self.val_f1s))\n",
        "        if _val_f1 > self.best_val_f1:\n",
        "            self.model.save(self.file_path, overwrite=True)\n",
        "            self.best_val_f1 = _val_f1\n",
        "            print(\"best f1: {}\".format(self.best_val_f1))\n",
        "        else:\n",
        "            print(\"val f1: {}, but not the best f1\".format(_val_f1))\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hd0wd_2wpeM",
        "colab_type": "text"
      },
      "source": [
        "#LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkXCAGBru-MP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LSTM_model(IsBinaryClass=True):\n",
        "  input_data = Input(shape=(83, 768,),dtype='float32')\n",
        "  merged = LSTM(100,input_dim=(83, 768,))(input_data)\n",
        "\n",
        "  merged=BatchNormalization()(merged)\n",
        "  if IsBinaryClass:\n",
        "    predictions = Dense(1, activation='sigmoid', kernel_initializer='random_normal')(merged)\n",
        "  else:\n",
        "    predictions = Dense(3, activation='softmax', kernel_initializer='random_normal')(merged)\n",
        "\n",
        "  model = Model(\n",
        "      inputs=input_data,\n",
        "      outputs=predictions)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llNef38Mwr-X",
        "colab_type": "text"
      },
      "source": [
        "#CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qejOh0gcXD0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CNN_binary_class(IsBinaryClass=True):\n",
        "  input_data = Input(shape=(83, 768,),dtype='float32')\n",
        "  conv1_x = Conv1D(100, 2,input_dim=(83, 768,), activation='relu')(input_data)\n",
        "  x_1 =GlobalMaxPooling1D()(conv1_x)\n",
        "  conv2_x = Conv1D(100, 3,input_dim=(83, 768,), activation='relu')(input_data)\n",
        "  x_2 = GlobalMaxPooling1D()(conv2_x)\n",
        "  conv3_x = Conv1D(100, 5,input_dim=(83, 768,), activation='relu')(input_data)\n",
        "  x_3 = GlobalMaxPooling1D()(conv3_x)\n",
        "\n",
        "  merged = concatenate([x_1, x_2,x_3],axis=-1)\n",
        "  merged = Dropout(0.3)(merged)\n",
        "  merged=BatchNormalization()(merged)\n",
        "  merged = Dense(units=128,kernel_initializer='random_normal')(merged)\n",
        "  merged=PReLU()(merged)\n",
        "#   merged = Dropout(0.2)(merged)\n",
        "  merged=BatchNormalization()(merged)\n",
        "  if IsBinaryClass:\n",
        "    predictions = Dense(1, activation='sigmoid', kernel_initializer='random_normal')(merged)\n",
        "  else:\n",
        "    predictions = Dense(3, activation='softmax', kernel_initializer='random_normal')(merged)\n",
        "\n",
        "  model = Model(\n",
        "      inputs=input_data,\n",
        "      outputs=predictions)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pVS8IQjY8L4",
        "colab_type": "text"
      },
      "source": [
        "#Set TARGET_TASK = 'subtask_a' , if you want to train task A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJalxyV-ONA",
        "colab_type": "code",
        "outputId": "3c61083f-11c6-4363-f2dd-65fd7f1e8084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "TARGET_TASK = 'subtask_c'  # Set TARGET_TASK = 'subtask_a' , if you want to train task\n",
        "Is_Binary_Class = True\n",
        "if TARGET_TASK == 'subtask_c':\n",
        "  Is_Binary_Class = False\n",
        "MODEL_SAVE_PATH='./output/CNN_Train_'+TARGET_TASK+'_4.hdf5' \n",
        "metrics = Metrics(MODEL_SAVE_PATH,\n",
        "                  validation_data=(bert_data['train'][data['validation'][TARGET_TASK+'_x_val']],data['validation'][TARGET_TASK+'_y_val']),\n",
        "                  IsBinaryClass=Is_Binary_Class)\n",
        "\n",
        "model=LSTM_model(IsBinaryClass=Is_Binary_Class)\n",
        "# model=CNN_binary_class(IsBinaryClass=Is_Binary_Class) # CNN model\n",
        "model.summary()\n",
        "# adam1 = keras.optimizers.Adam(lr=0.0001,decay=0.01) # for CNN model\n",
        "adam1 = keras.optimizers.Adam(lr=0.001,decay=0.0001) # for LSTM model"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(100, input_shape=(None, (83...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 83, 768)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               347600    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 348,303\n",
            "Trainable params: 348,103\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JbWo6k6xPcm",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4JVVcNsvK8R",
        "colab_type": "code",
        "outputId": "664f9c46-53b9-4f8f-ae83-d2975150b63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "BATCH_SIZE = 300\n",
        "NUM_EPOCHS = 5\n",
        "# class_weight = {0: 1.,\n",
        "#                 1: 2.}\n",
        "# class_weight = {0: 7.3,\n",
        "#                 1: 1.}\n",
        "# class_weight = {0: 1.,\n",
        "#                 1: 6.1,\n",
        "#                 2: 2.24\n",
        "#                }\n",
        "class_weight = {0: 1.,\n",
        "                1: 1.5,\n",
        "                2: 1.5\n",
        "               }\n",
        "model.compile(loss='binary_crossentropy',\n",
        "          optimizer= adam1,\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    x=bert_data['train'][data['train'][TARGET_TASK+'_x_train']],\n",
        "    y=data['train'][TARGET_TASK+'_y_train'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=[metrics],\n",
        "    # 每個 epoch 完後計算驗證資料集\n",
        "    # 上的 Loss 以及準確度\n",
        "    validation_data=(bert_data['train'][data['validation'][TARGET_TASK+'_x_val']],data['validation'][TARGET_TASK+'_y_val']),\n",
        "    # 每個 epoch 隨機調整訓練資料集\n",
        "    # 裡頭的數據以讓訓練過程更穩定\n",
        "    shuffle=True\n",
        ")\n",
        "#     class_weight=class_weight"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6099 samples, validate on 582 samples\n",
            "Epoch 1/5\n",
            "6099/6099 [==============================] - 5s 850us/step - loss: 0.5454 - acc: 0.7213 - val_loss: 0.5140 - val_acc: 0.7474\n",
            " — val_f1: 0.505084\n",
            "max f1;  0.5050839643341831\n",
            "best f1: 0.5050839643341831\n",
            "Epoch 2/5\n",
            "6099/6099 [==============================] - 4s 579us/step - loss: 0.4178 - acc: 0.8176 - val_loss: 0.4931 - val_acc: 0.7686\n",
            " — val_f1: 0.516072\n",
            "max f1;  0.5160716990482094\n",
            "best f1: 0.5160716990482094\n",
            "Epoch 3/5\n",
            "6099/6099 [==============================] - 4s 587us/step - loss: 0.3142 - acc: 0.8759 - val_loss: 0.5877 - val_acc: 0.7279\n",
            " — val_f1: 0.476753\n",
            "max f1;  0.5160716990482094\n",
            "val f1: 0.4767531615903236, but not the best f1\n",
            "Epoch 4/5\n",
            "6099/6099 [==============================] - 4s 592us/step - loss: 0.2190 - acc: 0.9226 - val_loss: 0.5766 - val_acc: 0.7904\n",
            " — val_f1: 0.504085\n",
            "max f1;  0.5160716990482094\n",
            "val f1: 0.504085085535031, but not the best f1\n",
            "Epoch 5/5\n",
            "6099/6099 [==============================] - 4s 584us/step - loss: 0.1627 - acc: 0.9425 - val_loss: 0.6205 - val_acc: 0.7572\n",
            " — val_f1: 0.481199\n",
            "max f1;  0.5160716990482094\n",
            "val f1: 0.48119919470930556, but not the best f1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk5y1kcP7WoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file_his = open('./output/CNNV5_train_hist_'+TARGET_TASK+'lstm_4.pickle', 'wb')\n",
        "# pickle.dump(history.history, file_his)\n",
        "# file_his.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMnswtAle7r6",
        "colab_type": "code",
        "outputId": "eb1b7d80-ea96-45cc-bcf7-098b835ea1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# history.history.keys()\n",
        "# history.history['f-score']=metrics.val_f1s\n",
        "# # history.history['best-fscore']=metrics.val_f1s\n",
        "# history.history.keys()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc', 'f-score'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wymw33Q0Ivzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQAssvDfx06k",
        "colab_type": "text"
      },
      "source": [
        "#Get The Best Model From Disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsLt1LJIxKQ0",
        "colab_type": "code",
        "outputId": "08375406-41ec-4294-a440-af1dd1824518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "      model=load_model(MODEL_SAVE_PATH)\n",
        "    # 若成功加载前面保存的参数，输出下列信息\n",
        "      print(\"checkpoint_loaded\")\n",
        "predictions = model.predict(bert_data['test'][TARGET_TASK])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint_loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf6QqxUZx9Nz",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate on Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIIIkwP8xWTt",
        "colab_type": "code",
        "outputId": "23dc451e-5b68-4b08-aab3-8a91bc5d1f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if TARGET_TASK == 'subtask_c'\n",
        "  predictions = np.argmax(predictions,axis=1)\n",
        "else:\n",
        "  predictions=predictions.round().astype('int').reshape(-1)\n",
        "print(predictions[:5])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWAWcDSP2lPc",
        "colab_type": "code",
        "outputId": "749f1b55-80f4-4e03-d97d-5d6764149407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "TEST_A_LABEL_PATH = DATA_SET_DIR+'labels-levela.csv'\n",
        "TEST_B_LABEL_PATH = DATA_SET_DIR+'labels-levelb.csv'\n",
        "TEST_C_LABEL_PATH = DATA_SET_DIR+'labels-levelc.csv'\n",
        "TEST_LABEL_PATH_DICT = {}\n",
        "TEST_LABEL_PATH_DICT['subtask_a'] = TEST_A_LABEL_PATH\n",
        "TEST_LABEL_PATH_DICT['subtask_b'] = TEST_B_LABEL_PATH\n",
        "TEST_LABEL_PATH_DICT['subtask_c'] = TEST_C_LABEL_PATH\n",
        "\n",
        "true_label = pd.read_csv(TEST_LABEL_PATH_DICT[TARGET_TASK],sep=',',header=None)\n",
        "true_label.head(3)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15923</td>\n",
              "      <td>OTH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60133</td>\n",
              "      <td>GRP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>83681</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1\n",
              "0  15923  OTH\n",
              "1  60133  GRP\n",
              "2  83681  IND"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klJx224b5zOp",
        "colab_type": "code",
        "outputId": "5cf8d5b3-66f1-4935-a9d8-3c210cd177b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test = true_label[1].apply(lambda x: classes_dict[TARGET_TASK][x])\n",
        "y_test = np.asarray(y_test).astype('int')\n",
        "y_test[:5]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFi4R08y56v3",
        "colab_type": "code",
        "outputId": "b13174ab-4662-49d7-bcb3-baf94ab10060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "testing_f1 = f1_score(y_test, predictions,average='macro')\n",
        "testing_acc = accuracy_score(y_test, predictions)\n",
        "print('testing data f1-score:',testing_f1)\n",
        "print('testing data accuracy:',testing_acc)\n",
        "# testing data f1-score: 0.4489635185344832\n",
        "# testing data accuracy: 0.49765258215962443\n",
        "# testing data f1-score: 0.5304134439163793\n",
        "# testing data accuracy: 0.6150234741784038"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing data f1-score: 0.54145361088442\n",
            "testing data accuracy: 0.6009389671361502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2rGkHZQ7eGi",
        "colab_type": "code",
        "outputId": "24a296c2-d656-4db8-a66c-387b8abc6c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# !zip lstm_bert_output_c4 output5/*"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: output5/CNN_Train_subtask_c_4.hdf5 (deflated 8%)\n",
            "  adding: output5/CNNV5_train_hist_subtask_clstm_4.pickle (deflated 20%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtusFN2IRtc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mv lstm_bert_output_c4.zip './gdrive/My Drive/NLP/Final-Project/output/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmsBEwPRWg-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mv ./output/CNN_Train_subtask_c_4.hdf5 ./output5/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kMxKCK-SDCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mv ./output/CNNV5_train_hist_subtask_clstm_4.pickle ./output5/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gS7kWX8Yl8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -q './gdrive/My Drive/NLP/Final-Project/output/cnn_outputt2_2.zip' -d ./output2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e1GFNZg1FRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best_m_p = './output2/output/CNNV5_Train_subtask_b.hdf5'\n",
        "# if os.path.exists(best_m_p):\n",
        "#       model=load_model(best_m_p)\n",
        "#     # 若成功加载前面保存的参数，输出下列信息\n",
        "#       print(\"checkpoint_loaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey0pp7OL3SwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhWDlFqK2N8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# emb_layer_model = Model(inputs=model.input,outputs=model.get_layer('concatenate_20').output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbyPJ1913cXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# emb_output = emb_layer_model.predict(data['test']['subtask_b'+'_x_test'])\n",
        "# # emb_output = emb_layer_model.predict(data['train']['subtask_b'+'_token_id'])\n",
        "# print(emb_output.shape)\n",
        "# print(emb_output[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wy9grzf35M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('cnn_subtaskb_xtest_300',emb_output)\n",
        "# np.save('cnn_subtaskb_ytest',data['train']['subtask_b'+'_y'])\n",
        "# np.save('cnn_subtaskb_ytest',y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0lHVZ5J3yTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# emb_output = emb_layer_model.predict(data['train']['subtask_b'+'_token_id'])\n",
        "\n",
        "# print(emb_output.shape)\n",
        "# print(emb_output[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM7dLvLQ2s6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "df5c8947-c54f-4f4f-956d-feced7e0f03b"
      },
      "source": [
        "# print(data['train']['subtask_b'+'_token_id'].shape)\n",
        "# data['test']['subtask_b'+'_x_test'] = text2id(tokenizer, data['test']['subtask_b'].tweet_exp)\n",
        "# print(data['test']['subtask_b'+'_x_test'].shape)\n",
        "# TEST_A_LABEL_PATH = DATA_SET_DIR+'labels-levela.csv'\n",
        "# TEST_B_LABEL_PATH = DATA_SET_DIR+'labels-levelb.csv'\n",
        "# TEST_C_LABEL_PATH = DATA_SET_DIR+'labels-levelc.csv'\n",
        "# TEST_LABEL_PATH_DICT = {}\n",
        "# TEST_LABEL_PATH_DICT['subtask_a'] = TEST_A_LABEL_PATH\n",
        "# TEST_LABEL_PATH_DICT['subtask_b'] = TEST_B_LABEL_PATH\n",
        "# TEST_LABEL_PATH_DICT['subtask_c'] = TEST_C_LABEL_PATH\n",
        "# temp_task = 'subtask_b'\n",
        "# true_label = pd.read_csv(TEST_LABEL_PATH_DICT[temp_task],sep=',',header=None)\n",
        "# y_test = true_label[1].apply(lambda x: classes_dict[temp_task][x])\n",
        "# y_test = np.asarray(y_test).astype('int')\n",
        "# print(y_test.shape)"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4400, 66)\n",
            "all sequence length are  66\n",
            "(240, 66)\n",
            "(240,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F0NNDxA25DV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2529ff6b-6294-4b74-e504-a5754b41e1e9"
      },
      "source": [
        "# data['train']['subtask_b'+'_y']"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 1., 1., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWAFEaSlOByK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "044a4cdd-07ea-4a20-81a7-c061e59e667a"
      },
      "source": [
        "# !zip cnn_bert_output1 output/*"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: output/CNN_Train_subtask_a.hdf5 (deflated 9%)\n",
            "  adding: output/CNN_Train_subtask_b_2.hdf5 (deflated 9%)\n",
            "  adding: output/CNN_Train_subtask_b_3.hdf5 (deflated 8%)\n",
            "  adding: output/CNN_Train_subtask_b_3_TRUE.hdf5 (deflated 8%)\n",
            "  adding: output/CNN_Train_subtask_b_4.hdf5 (deflated 9%)\n",
            "  adding: output/CNN_Train_subtask_b.hdf5 (deflated 9%)\n",
            "  adding: output/CNN_Train_subtask_c.hdf5 (deflated 9%)\n",
            "  adding: output/CNNV5_train_hist_subtask_alstm_1.pickle (deflated 40%)\n",
            "  adding: output/CNNV5_train_hist_subtask_bcnn_3.pickle (deflated 42%)\n",
            "  adding: output/CNNV5_train_hist_subtask_bcnn_4.pickle (deflated 43%)\n",
            "  adding: output/CNNV5_train_hist_subtask_bcnn.pickle (deflated 38%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmCH1od4bjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mv cnn_bert_output1.zip './gdrive/My Drive/NLP/Final-Project/output/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13fYqq-W8yWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mv ./output/CNN_Train_subtask_b_4.hdf5 ./output/CNN_Train_subtask_b_3_TRUE.hdf5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKx6GpxPNu1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "d3b49795-956a-4169-e2e2-f248bf812982"
      },
      "source": [
        "# !ls ./output/"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN_Train_subtask_a.hdf5    CNN_Train_subtask_c.hdf5\n",
            "CNN_Train_subtask_b_2.hdf5  CNNV5_train_hist_subtask_alstm_1.pickle\n",
            "CNN_Train_subtask_b_3.hdf5  CNNV5_train_hist_subtask_bcnn_3.pickle\n",
            "CNN_Train_subtask_b_4.hdf5  CNNV5_train_hist_subtask_bcnn.pickle\n",
            "CNN_Train_subtask_b.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP-XT5ZLWrFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}